\documentclass[10pt, conference, letterpaper]{IEEEtran}

\usepackage{times}		%% native times roman fonts
\usepackage{parskip}		%% blank lines between paragraphs, no indent
\usepackage[pdftex]{graphicx}	%% include graphics, preferrably pdf
\usepackage{amsmath}
\usepackage[pdftex]{hyperref}	%% many PDF options can be set here
\usepackage{amsthm,amssymb}
\usepackage{mathtools}
\usepackage[utf8]{inputenc}
\usepackage{stfloats}
\pdfadjustspacing=1		%% force LaTeX-like character spacing

\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\hypersetup{
  pdfauthor = {Jos\'e Leal Domingues Neto},
}

\title{User-Level Location Aware Decision Engine for Mobile Computational Offloading}
\author{Jos√© Leal Domingues Neto \\ DCC, Universidade Federal de Minas Gerais \\ \href{mailto:joseleal@ufmg.br}{joseleal@ufmg.br}}

\begin{document}
\maketitle

\begin{abstract}
  Mobile computational offloading is a promising field in the area of mobile computing. Its benefits range from better responsiveness to less energy dependent applications. One of the big challenges in decent offloading frameworks is context-dependant dynamic evaluation of whether offloading at the moment is the right decision or not. This work proposes a location dependent decision engine for better bandwidth, RTT and execution time prediction. This approach assesses situations that were not foreseen in similar projects and promises better approximations, thus better decisions. This work also models the problem to give the user the possibility of weighting between energy and responsiveness when offloading.
\end{abstract}

\section{Introduction}
  Mobile devices cope with the difficult trade-off of providing high-end rich features and maintaining reasonable response time and battery life. Mobile devices follow the well known Moore's law, seeing processing and graphics power, storage capacity and high speed connectivity growing at fast pace while battery life does not tail such trend, becoming therefore a clear bottleneck \cite{Cuervo:2010:MMS:1814433.1814441}. Mobile computational offloading migrates pieces or possibly whole computational programs from the local processor to a less power limited platform e.g. a cloud computing environment \cite{Scavenger:5466972}. Such operation may reduce response time and energy consumption.  

  One of the most important tasks in offloading, is the assessment of whether the device will profit by executing code locally or remotely. This dictates the success rate of the technique: If no energy and/or no time were saved in the operation, offloading does not make sense anymore. Most offloading frameworks predict the available bandwidth and execution speed (TODO REF). Some of them oversimplify the problem modeling to overcome this problem.
  
  This work tries to give better prediction and inputs to the problem modeling, using location awareness.
  
  Location can give important data when predicting some of the parts of the decision engine. Location directly affects properties such as: Energy consumption, signal processing, QoS and bandwidth. This means that the perception of the user regarding the application is also location dependent. When using the mobile device in an area with poor connectivity, the antenna has to be kept in a high energy state. The same applies for WiFi and mobile data. According to \cite{Schulman10bartendr:a}, six times more energy can be spent when transferring data in such situations. Bandwidth and RTT (round-trip-time) also vary according to location. Existing frameworks do not account for location, proving therefore poor predictions in many cases.
  
  At last, since location is an expensive computation, this project aims to use already gathered location data from other applications. This means that the decision engine by itself will not use extra resources when determining geoposition. Since using cached positioning may lead to imprecisions, geoposition is optional in the proposed model.

  This work also aims to solve one big issue with predictions: The runtime-execution of methods. Many of the projects use static profiling to assess code execution. The used metric is the number of instructions or simply the empirically measured execution time. This approach has many problems, as method execution is very dependent on inputs and the algorithm itself. Our framework treats the method as a black-box adjusting execution time as the input changes.

  \section{Related Work}

  Much was invested in dynamic and static evaluation of offloading. MAIU \cite{Cuervo:2010:MMS:1814433.1814441} offers a dynamic profiling mechanism, so called Maui Solver. It collects data for later assessment, taking RTT and bandwidth into account. These values are gathered and used as averaged estimates. We use a similar approach for these values when location cannot determined. CloneCloud \cite{Chun:2011:CEE:1966445.1966473} uses CPU activity, display state (on or off) and network state when deciding to offload. As said before, we will use network state as well as inputs to our decision engine. ThinkAir \cite{kosta2012thinkair} has a comprehensive set of attributes monitored. They check battery level, data connectivity presence, connection type (WiFi, 3G, GPRS, UMTS), CPU activity, WiFi State (On/Off), perceived bandwidth, RTT and many others. We use a simpler model, reducing this set to avoid overhead. ThinkAir uses historical execution data for later predictions. Our decision engine uses more sophisticated algorithms to predict this, interpolating values as input varies.

  In the server-side, the COSMOS \cite{Shi:2014:CCO:2632951.2632958} project aims to manage the remote platform resources diminishing costs by effectively allocating and scheduling offloading requests to reduce monetary cost per request. This is an important issue, as we aim for a feasible solution, also in financial means. Other approaches aim the fast provisioning of servers to respond to elastic demand of computational offloading frameworks \cite{Ha:2013:JPC:2462456.2464451}.

  Other works like \cite{6162380}, borrow the model from an financial background, modelling therefore the offloading decision as a cost-effective option. This means that it will favor the most profitable choice at the moment, which in this case will be smartly modelled according to the problem. This work has a similar framework, fitting the decision in utility functions and choosing the local optimal value in a greedy fashion. Scavenger \cite{5466972} creates a pervasive computing environment to make offloading possible. In this scenario, the client looks for services running in other devices nearby and schedules tasks on them using RPC.

  For predicting future inputs \cite{Balan:2003:TRE:1066116.1066125} \cite{Cuervo:2010:MMS:1814433.1814441} \cite{kosta2012thinkair} draw a linear model of resource usage to extrapolate future values. This allows an approximate prediction of inputs when they are not readily available or stale, such as bandwidth and method execution time. This project improves these predictions by interpolating these values using a spline, thus receiving a smaller delta of its real value at the time.

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.35\textwidth]{imgs/system.jpeg}
  \caption{Structure of an Offloading Framework}
  \label{fig:systemDiagram}
\end{figure}

  \section{Proposed solution} \label{sec:design}

  This project focuses on the offloading decision, however we describe the components of an offloading system before diving into the details of our solution. \ref{fig:systemDiagram} shows the components of an offloading framework. The instrumentator, will intercept and instrument the candidate methods for offloading. It will also store results and partial steps of these instrumentations. The decision engine, analyses inputs and predicts the output of the methods. Using that information it decides if offloading should be performed. The connector, takes care of remote connectivity and serialization/deserialization of objects.  The scope of this work is limited to the decision engine.

  The proposed decision engine employs an utility function, where the user prioritizes either reducing execution time or energy consumption. A constant, $\alpha, 0 \leq \alpha \leq 1$ is used for this purpose. A smaller $\alpha$ will make the decision tend towards saving energy. On the other hand, a bigger alpha will improve responsiveness. 

  The main equation for the boolean decision is as follows:
  \begin{equation} \label{eq:localutility}
    L(M) := \alpha . tm_{l}(M) + (1-\alpha) . en_{l}(M) 
  \end{equation}
  \begin{equation} \label{eq:remoteutility}
  \begin{multlined}
    R(M) := \alpha . tm_{r}(M) + (1-\alpha) . en_{r}(M) 
  \end{multlined}
  \end{equation}

  Where $M$ is the offloading candidate, $L(M)$ is the estimated utility of local execution, while $R(M)$ is the utility of remote execution. The utility employs functions to assess the time ($tm$) and energy ($en$) of the execution of $M$. Those functions will be detailed in section \ref{sec:time} and section \ref{sec:energy} respectively. To decide if the method should be offloaded, we compare the utility of local and remote executions as follows: 

  \begin{equation}
    decision(M) := R(M) < L(M)
  \end{equation}

  \subsection{Time} \label{sec:time}
  Time can be a critical part to be computed. Mobile devices are developed to run several processes simultaneously. On top of that, the method to be offloaded is still a black-box and static profiling of code not always can give us useful hints. That means that better techniques ought to be developed.
  
  This paper introduces the concept of input assessment to try to understand the execution time of a method given its arguments. The concept is simple: A list of arguments of any type is converted to a unique numerical variable $assess(args)$. This variable will be directly proportional to the method's complexity. That means that given bigger assessments, the framework can expect that the method will take more time to finish. This is done by analysing annotations provided by the application developer, that are read in execution time. The annotations are used as a hint for the engine to point variables that are critical for the execution time of the method. Hard coded types of variables are already available: if an \textit{integer} is annotated, its real value is summed to the assessment. If a \textit{string}, \textit{list} or \textit{set} is annotated, then its length is counted. If this behaviour needs to be changed for a specific variable, or if non-standard types are used (such as custom classes), then the annotation can accept a class as argument, which should implement an interface, converting this argument to a numerical value. The assessment is made at run-time. 

  The assessment value is then compared with the empirical execution time of the method. After five executions, a rough approximation function of the method's asymptotic complexity can be made. The Akima Spline function was used as interpolator.

  Therefore, $tm_{l}(M)$ is defined as follows:

  \begin{equation} \label{eq:timelocal}
    tm_{l}(M) := interpolate(M, Asm(M.args))
  \end{equation}
  \begin{equation}
    Asm(A) := \sum_{a \in A} assess(a)
  \end{equation}

  For remote execution the same approach is made, gathering the execution time from the server and interpolating this value. On the remote platform we must take into account the additional uploading and downloading of the method's serialized arguments. To calculate this, location aware historical empirical data is used from past executions. Every time the method runs, location aware bandwidth is calculated and added to this database. The rule of thumb is: when on WiFi, bandwidth values from the same \textit{SSID} are used. When on mobile data, the tower's LAC/CID is recorded, together with its noise to ratio to later calculate expected bandwidth values.

  To update the bandwidth values in the database, the formula \ref{eq:bandwidthup} is used to avoid drastic changes.

  \begin{equation} \label{eq:bandwidthup}
    b(t+1) = b(t) . (1-\beta) + value . (\beta), 0 \leq \beta \leq 1
  \end{equation}

  Finally the remote execution time follows equation \ref{eq:timeremote}.

  \begin{equation} \label{eq:timeremote}
    tm_{r}(M) := interpolate(M, Asm(M.args)) + trf(M.args)
  \end{equation}
  \begin{equation}
    trf(o) := size(o) / bandwidth
  \end{equation}

  \subsection{Energy} \label{sec:energy}

  Many efforts were made to calculate energy from the mobile platform. The question still remains open, since there is still no precise way to calculate the method's energy consumption from a user-level point of view. The engine cannot gather this data precisely, therefore two assumptions are made: (a) Time is energy. So energy spent is directly proportional to time spent. (b) WiFi/Mobile Data are comparable to CPU. That means that using a modifier constant one can find a direct mapping between these values. This facilitates normalization of the utility function, since it results on the same unit (time) when comparing \cite{6606420}.

  The local energy consumption is therefore a simple sum of the CPU time spent, together with any data transferred by the method. This is simply collected at runtime and similarly interpolated, as done with execution time. In fact, two splines are created: one for execution time, the other for transferred data. This is done because of the black-box approach taken by this work. 

  \begin{equation} \label{eq:energylocal}
    en_{l}(M) := \\ tm_{l}(M) . C_{cpu} + dataTransferTime(M)
  \end{equation}

  Where $C_{cpu}$ is the modifier constant and $dataTransferTime(M)$ will deliver the interpolated value of the function's transfer time of data according to its assessment. The remote counter-part of this equation contains similarly a constant $C_{radio}$ which normalizes the energy spent in radio, being the sum of transfer times of the method's serialized arguments and its result.

  \begin{equation} \label{eq:energyremote}
    en_{r}(M) := (trf(M.args) + trf(M.result)) . C_{radio}
  \end{equation}

  \subsection{Choice of constants}
  As seen in section \ref{sec:energy}, the two modifier constants $C_{cpu}$ and $C_{radio}$ are used. They are necessary for a real mapping between energy and time. Their values are tabular and should be extracted from the data sheet of the devices. In a later time, with the values in hand, the framework can check the device's manufacturer and set the constants according to the data provided.

  \subsection{Location aware bandwidth prediction}
  When on mobile data, the framework acts differently in order to predict real values for the current bandwidth. A slightly modified Shannon's equation is used, together with the current noise-to-signal ration acquired in real time from the mobile device. More formally, when receiving the perceived bandwidth $bw$, and the noise-to-signal ratio $SNR$, the LAC/CID from the current radio tower connected, a value $S$ is recorded in the database defined by:

  \begin{equation} \label{eq:shannonbw}
    S := bw / log_2(1 + SNR)
  \end{equation}

  The stored result will be then later retrieved to provide a location aware bandwidth, that will better describe the conditions at that location, taking into account the signal quality and tower wide band capacity. For retrieving the bandwidth in a later time, the following reverse equation is used:

  \begin{equation} \label{eq:shannonbw_reverse}
    S_{reverse} := S / log_2(1 + SNR)
  \end{equation}

  This way, a new educated guess is retrieved according to new values of $SNR$ and $S$ at the tower. Upon new data, this value is updated maintaining therefore the most actual state.

\begin{figure*}[!t]
  \centering
  \includegraphics[width=1\textwidth]{plots/executions/executions.png}
  \caption{Results of experiments using $\alpha = 0.35$. One can see how the remote platform has a milder execution-time curve. Summing the value of RTT+Remote-execution one can devise the point where offloading is the optimal choice.}
  \label{fig:exectime}
\end{figure*}

  \section{Preparation for Experiments}
  For experimenting the effectiveness of the decision engine, a full-sized Android application for calculating the \textit{Fibonacci numbers} was created as proof-of-concept. The decision engine described in this work was used together with an Offloading Framework, which lies outside the scope of this project, to test predictions and correctness of the responses.

  \subsection{Offloading framework}
  The offloading framework used is a parallel work in development. It has a simple structure: (a) Post-compiler bytecode manipulator to inject method interceptors; (b) Remote execution platform running a Dalvik Virtual Machine \cite{ehringer2010dalvik}; (c) Offloading client library for server connection handling, serialization and deserialization, decision engine and instrumentation.

  The proposed decision engine is plugged into (c). The remaining parts of the framework will be briefly explained for contextualization, but its scope differ from this project.

  The post-compiler bytecode manipulator receives an Android APK application as input and outputs a modified APK with the offloading stubs. It looks for the annotation "OffloadCandidate" in methods, creates a renamed copy of them, and modifies its body adding the necessary calls for intercepting and triggering the decision engine. It then invokes the local or remote execution according to the output of the decision engine. The remote platform is composed of one or more servers running a DalvikVM. In each instance a server application receives pings and requests of clients. In case of requests for remote execution it receives the serialized arguments, runs the given method and returns the output.

  \subsection{Fibonacci Numbers application}
  An android application for calculating the \textit{fibonacci numbers} was created as a proof-of-concept. It offers basic functionality and can be easily attested for correctness and effectiveness, as its output and behaviour is predictable and limited. Among other reasons its development is easily portable to other languages and very fast to be implemented. A simple wrap application was created having a single method determining the Fibonacci numbers. This was annotated as an "OffloadCandidate" in order to be modified by the post-compiler. The user can then choose the number to be calculated and press a button to receive the result.

  \begin{lstlisting}[caption=Fibonacci Numbers calculation, label=lst:fibonacci]
	@OffloadCandidate
	public int fibonacciRecusion(
      @RelevantParameter(weight=1)Integer number){

        if(number == 1 || number == 2){
            return 1;
        } 
        return fibonacciRecusion(number-1) + fibonacciRecusion(number -2); //tail recursion
    }
  \end{lstlisting}

  In code listing \ref{lst:fibonacci} one can see the naive approach of the method. This was made on purpose to easily yield outrageous intense calculation with already small inputs.

  \subsection{Remote platform}
  Several instances can be created to serve requests according to demand. For this purpose, one instance was spun to be liable for requests. To diminish costs, a \textit{Droplet} snapshot in DigitalOcean's infrastructure \cite{digitalocean} was prepared. DigitalOcean offers IaaS (infrastructure as a service) virtual machines that are charged only for time used. That means that a powerful machine can be created, used for some hours and then brought down with very low monetary costs. DigitalOcean offers a 3GHz Intel Hexcore processor to all its Droplets. The VM sports 4GB RAM to run the DalvikVM with the server application in it. The DalvikVM of choice was a regular android AVD emulator image. It was created using the Android AVD Manager \cite{androidavd}, which offers full-fledged control over the emulator with the \textit{adb} tool set. Additionally it can run headless and allows remote access via ssh.

  \subsection{Mobile platform}
  The mobile cellphone used for the tests was a Samsung Galaxy S5, which is powered by a Snapdragon 801 processor having a 2.5 GHz quad-core CPU and 2GB RAM. We can see that we are almost facing a technology convergence regarding computing power between the platforms. The only difference is that depending on context, the Snapdragon chipset can rule down CPU clock to save battery, leading to limited performance.

  \section{Experiments}
  Using the Android application one can create several experiments, varying the $\alpha$ parameter, network state and location. The experiments were all made under real circumstances, not being simulated or emulated in any case.

  \subsection{Experiment 1: $\alpha = 0.35$}
  The alpha value, that can be recalled from section \ref{sec:design} was set to an arbitrary but concious value $\alpha = 0.35$. This means basically that it will favour saving energy rather than trying to give more responsiveness. This value was chosen after some testing and it proved to deliver the best ratio between energy and responsiveness. Further testing was made using different parameters. This can be seen later in this section. The execution-time results can be seen in \ref{fig:exectime}. They were obtained by running the application inside a WiFi connectivity environment.

  According to figure \ref{fig:exectime}, one can see the simple and predictable behaviour of he fibonacci function regarding time execution. It is a quadratic function, almost doubling its execution time as input grows. At first, with small numbers, the local mobile platform can easily outrun the remote platform, delivering near zero-delay results, thus making a clear point when \textbf{not} to offload. The blue line represents the RTT at the time. Summing the value from the RTT with the value of the remote execution-time bar, a longer time is obtained. That means we are not profiting neither in energy nor responsiveness. Starting from Fibonacci number 36, one can see that summing the values from the remote execution and RTT yield a smaller value than the local execution, portraying a clear advantage over local execution.

\begin{figure*}[!t]
  \centering
  \includegraphics[width=1\textwidth]{plots/threshold/executions.png}
  \caption{Line plot summing RTT and Remote time execution versus local execution. Regions are created in the graph, representing the offloading decision.}
  \label{fig:graphregions}
\end{figure*}

Creating a spline for predicting the curve proved to be something essential from an architecture point of view. This made the evaluation of the expected value easier. After some testing was clear, how a decision region was formed. Since the assessment function is a monotonically increasing function, execution time will grow directly proportionally to it. It follows the idea that after some point, there is no need anymore to evaluate the curve completely. One can see that if we have two assessments $A$ and $A'$, such that $A \leq A'$, respective execution times $e$ and $e'$ will be $e \leq e'$ as a rule. One need to simply find for some $A'$ and some $A$ where $e + RTT \leq e'$. If found, it means that offloading will be the optimal solution at the time. In figure \ref{fig:graphregions} the decision regions are shown separated by a vertical line. The line's assessment point is the $A$ for all $A'$ above that point.

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.5\textwidth]{plots/map/map-plot.png}
  \caption{Map built with the help of Google maps API to picture execution positions.}
  \label{fig:mapplot}
\end{figure}

  \subsection{Location aware bandwidth}
  As claimed in this work, a location aware entity would profit greatly when predicting bandwidth. Moving around the city, our experiments attest this behaviour. The application was used while on movement, calling the offload candidate method with 4G and WiFi connectivity to observe how bandwidth changes according to location. In figure \ref{fig:mapplot}, selected points are shown. One can observe several bandwidth variations, which were updated using the formula \ref{eq:bandwidthup}. In the case of connectivity type \textit{WiFi}, the previous bandwidth values from the same SSID or in a radius from 500 meters is taken. That means that we have a shallow variation starting from the known previous value, thus not needing to wait for a request to actually complete before predicting bandwidth.
  
  The system behaves a little different when facing non-wifi connectivity type: in this case the point is recorded and requests in the range of 1km will be considered. Point $p_3$ and $p_4$ seen in the image present small difference in their bandwidth values, which can prove the idea that close points will have close values. Although location could already help very much at some point, we must note that sometimes, location was stale and did not help much. To maintain a non-energy hungry behaviour, location is only updated when the device decides so. That means the user may have moved some kilometers before a new position can be acquired. In an urban environment that may not be a problem since antenna coverage is decent and is maintained across some kilometers.

  \subsection{Alpha parameter}

  Finding the correct parametrization for each application is an important task. Much can be discovered by simply setting scattered alpha values and seeing the outcome. In this case, a turning point was found: An assessment value was discovered, where different parametrization yield different boolean answers. Another discovery was made: bandwidth and CPU availability play also a significant role. In table \ref{table:alpha} we have 4 executions, each with the same assessment, but run in different conditions. The \textit{Remote} and \textit{Local} column represent respectively the result of the utility functions \ref{eq:remoteutility} and \ref{eq:localutility}. The smaller value will be chosen as boolean answer. As one can see, although the alpha value did not change, a different answer was received at the end. This happens because of the current network state (bandwidth) and the CPU activity at the time. In line 2, the local utility function has a smaller value, thus meaning that CPU was relatively free and computed the method faster.

  \begin{table}[!t]
  \centering
  \caption{Utility function evaluation}
  \label{table:alpha}
  \begin{tabular}{rcrrc}
    \textbf{Alpha} & \textbf{Assessment} & \textbf{Remote} & \textbf{Local} & \textbf{Bandwidth} \\
   0.1 & 32  & 342.52 & 507.56 & 0.409  \\
   0.99 & 32  & 478.69 & 381.0 & 0.409  \\
   &  & & &  \\
   0.1 & 32  & 1231.01 & 507.0 & 0.11  \\
   0.99 & 32  & 379.28 & 507.0 & 0.433  \\
  \end{tabular}
  \end{table}

  \section{Results}
  According to the experiments, the decision engine has proved itself to deliver better predictions and more precise outputs according to the situation. It can be flexible enough to serve different cases, adapting according to location and connectivity type. It also gives power to the developer, allowing him to tailor the assessment values and shape the function spline. Together with the offloading framework, it may improve application responsiveness and diminish battery usage.
  
  As we can see from the execution times, little overhead and calculations are needed to determine whether offloading is a feasible option depending on context. The decision engine showed very good performance on simple calculations, although further testing was not possible due to time restrictions. Positioning was obtained by getting the last known location. This did not work perfectly all the time, returning stale geopositiong, misleading bandwidth calculation at some points.

  Predicting the size of the result has also an impact in behaviour. As one can imagine that this is not really important in the case of our \textit{Fibonacci numbers}, but can be a turning point when compared to some method that may delived a complex object. The remote platform could try to do a regression based on historical data and return the approximated result size in a later point. This is left for future work.

  Another point to be discussed is the parametrization of energy spent in CPU, WiFi and radio. With the evolution of chipsets, each component might change drastically its energy footprint. Assuming that WiFi and CPU spend the same amount of energy is a very crudal generalization and need to be changed to accept parametrized values. 

  \section{Future work} \label{sec:futurework}

  The conception of this decision engine is still a moving target and many parts are due to be done. The experimentation section of this work is still left undone, together with proper evaluation. Better location acquisition needs to be researched, since the geoposition used is still the last known user's location. This can lead to unstable decisions. Other metrics should also be added to equations, such as the serialized method result size.

  Security research also needs to be addressed, since nothing was said about secure communication channels or user authentication.

  We can explore more the concept of assessment, being able to cache results according to the method's assessment value. Given some assessment $a$, one can try to predict the outcome of the decision based on historical data. Additionally, since the real word is very difficult to be modelled, some entropy can be added to the engine, giving the system some beneficial randomization.  

  \section{Conclusion}

  The decision engine of an offloading framework plays a central role. It will affect directly the success rate of the framework. This work tries to develop a novel approach including location aware metrics to its formulas. Execution time was improved by giving the developer tools to advise about instrumented code.

  According to experiments, the decision engine can deliver more precise results by using location data and black-box analysis of code. Depending on method and the remote platform's speed, responsiveness and energy footprint can be improved. There is still room for improvement in the area of security, statistics and analysis. This was left for future work.



\bibliographystyle{unsrt}
\bibliography{parcial}

\end{document}
